%!TeX root = ../index.tex

\section{Introduction}\label{sec:introduction}

\section{Reactive \acrlongpl{is}}

\citeauthor{boner_reactive_2014} describe how modern \glspl{is} of different domains all exhibit similar qualities.
\citetitle{boner_reactive_2014} summarizes those qualities and dubs the systems which feature them \enquote{reactive} \parencite{boner_reactive_2014}.
% Put into the context of the quality dimensions defined by the \citetitle{iso_25010_2011} standard,
% the reactive qualities cover only \enquote{Performance Efficiency} and \enquote{Reliability}.
% While the other quality dimensions of the \citeauthor{iso_25010_2011} standard are certainly also important, this work focuses on the two mentioned before.\todo{improve sentence flow}
According to \cite{boner_reactive_2014}, reactive systems are \emph{responsive}, meaning that they have low upper ceilings for their response times and meet them consistently.
Additionally, reactive systems remain responsive in the face of failure.
They are \emph{resilient}.
Lastly, a system needs to remain responsive under varying work-load.
It needs to be \emph{elastic}, to be reactive.

All of these qualities cover the technological perspective on reactive systems.
However, there is a business perspective to the term, too.
Reactive systems also need to be flexible.
They need be able to to respond and change quickly according to changing requirements.
For example due to changing customer or market needs.
\citeauthor{beck2001agile} describe this quality as part of their \citetitle{beck2001agile} \parencite{beck2001agile}.
The text will refer to this quality as \emph{flexibility} from here on.

Different design and work patterns have emerged to create reactive systems.
The following section explores an increasingly popular one: microservice architecture \parencite{loukides_microservice_adoption_2020}.

\subsection{Microservice Architecture}

\citeauthor{richardson_microservices_2019} defines the microservice pattern as \enquote{an architectural style that functionally decomposes an application into a set of services} \parencite[11]{richardson_microservices_2019}.
The services are supposed to be loosely coupled, independently deployable, and scalable.
They are also small enough to be developed by a single, largely autonomous team.
The compartmentalization of the application also allows for it to tolerate partial failure.
If a single service fails, the rest of the application may stay partially functional.
\parencite[14f.]{richardson_microservices_2019}
These qualities contribute to the application's overall elasticity, resilience, and flexibility.

It is pointed out by \citeauthor{fowler_microservices_2014} that microservices commonly integrate with each other via synchronous communication protocols like \glspl{rpc} or HTTP \parencite{fowler_microservices_2014}.
This introduces different forms of coupling to the system.
First, the services need to know routes via which they can reach another directly.
Second, the services need to know not only the structure of the data they exchange but also each others' \gls{api}.
Last, the synchronous nature of the communication couples the services in the dimension of time.
A service either completes a request in time, or the request reaches a timeout.
It is for this reason that many applications rely on asynchronous, message-driven communication via \gls{mom} \parencite{fowler_microservices_2014}.

\subsection{\acrlong{mom}}

\cite{boner_reactive_2014} defines one additional quality of reactive systems: \emph{message-driven}.
Message-driven systems \enquote{rely on asynchronous message-passing to
establish a boundary between components that ensures loose coupling, isolation,
location transparency, and provides the means to delegate errors as messages.} \parencite{boner_reactive_2014}
As shown in figure \ref{fig:reactive-traits}, the message-driven quality forms the foundation for achieving all other qualities of a reactive system.

\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{reactive-traits.png}
  \caption{The Qualities of a Reactive System \parencite{boner_reactive_2014}}
  \label{fig:reactive-traits}
\end{figure}

\cite{banavar_case_1999} coined the term of \acrlong{mom} for dedicated software components meant to integrate disparate applications in a distributed system by facilitating asynchronous message-passing between them.
\citeauthor{banavar_case_1999} based their work in part on the publish/subscribe semantics described in \cite{oki_information_1993} which are still present in the more comprehensive exploration of \gls{mom} in \cite{curry_message-oriented_2004} and prevail to this day.

There exist multiple established protocols which implement the patterns of \gls{mom} like AMQP and MQTT \parencites{amqp}{mqtt} and software solutions implementing the protocols (also called message brokers) like RabbitMQ or Mosquitto \parencites{rabbitmq}{mosquitto}.
While implementation details and terminology might vary between protocols and brokers, the general patterns are the same.

Clients \emph{publish} packets of data called \emph{messages} to a logical communication channel managed by the broker.
From here on, the text refers to such communication channels as \emph{topics}.
Clients can also \emph{subscribe} themselves to these topics and receive messages published to them.
Message delivery may be transient or persistent, meaning that messages are either delivered immediately to the subscribers of a topic by the broker or that the broker stores the messages in subscriber-specific, first-in-first-out queues.
Queues allow subscribers to consume messages at their own pace and become temporarily unavailable without losing messages.
\parencite{curry_message-oriented_2004}

However, one popular message broker does things a little differently.
The following section examines the ways in which Apache Kafka implements the patterns of \gls{mom}.

\subsection{Apache Kafka}

The LinkedIn employees \citeauthor{kreps_kafka_2011} introduced their new \enquote{Event Streaming Platform} \emph{Kafka} in \citeyear{kreps_kafka_2011} \parencite{kreps_kafka_2011}.
Over a decade later, the project has since joined the Apache foundation and \enquote{[m]ore than 80\% of all Fortune 100 companies trust, and use [it]} \parencite{apache_software_foundation_apache_nodate}.

\citeauthor{kreps_kafka_2011} describe Kafka as \enquote{[\ldots] a novel messaging system for log processing [\ldots] that combines the benefits of traditional log aggregators and messaging systems.} \parencite{kreps_kafka_2011}.
Similar to traditional \gls{mom}, \emph{producers} publish messages to logical communication channels called topics and \emph{consumers} can subscribe to topics to consume messages.
In contrast to traditional \gls{mom} however, topics are backed by one or more \emph{partitions}, an append-only data structure stored on the hard drive of a server running Kafka called a \emph{broker}.
Therefore, Kafka persists all messages sequentially for a configurable time even if the brokers restart.
Producers address messages to a topic but actually always publish them to a partition.
To which partition they publish a message if there are multiple is determined by a partitioning algorithm.
Partitions can also have replicas on other brokers, making Kafka tolerant against the failure of single servers.
\parencite{kreps_kafka_2011}

A message in Kafka does not have a dedicated ID.
Instead, a message's logical offset within its partition identifies it.
Consumers consume messages sequentially and move a pointer to the next offset after they have processed the message at the current position.
Furthermore, consumers can form groups in which no two members subscribe to the same partition, thereby achieving a simple load balancing mechanism.
That also makes the number of partitions that a topic has the maximum degree of parallelism with which consumers can consume its messages.
\parencite{kreps_kafka_2011}

Kafka is not only useful for processing the kinds of log data that are proposed by \citeauthor{kreps_kafka_2011}: activity (e.~g. user logins, clicks, \enquote{likes}, \ldots) and operational data (e.~g. \gls{cpu} or disk utilization, HTTP requests, \ldots) \parencite{kreps_kafka_2011}.
\citeauthor{stopford_designing_2018}, for example, outlines how Kafka can serve as the basis of a business application's entire state management and internal communication.
His design implements concepts from the \gls{ddd} community like Event Sourcing \parencite{fowler_event_sourcing_2005} and \gls{cqrs} \parencite{fowler_cqrs_2011} with Kafka's log-based messaging at the center \parencite{stopford_designing_2018}.
The potential use cases of Kafka are therefore plentiful.

\section{Schemas}

This section examines schema technologies and how they can be used to make a distributed \gls{is} more reactive.
For the latter purpose, the text proposes a hypothetical design of an \gls{is} using microservices and Kafka as a \gls{mom}.
This example serves as a basis to analyze what advantages the introduction of Apache Avro as a schema technology provides, what new problems it causes and how to address them.\todo{reason for choosing avro/kafka}

Schemas are structured documents that describe the structure of other documents.
Schema technologies like XML schema, JSON schema, Apache Avro, or Google's Protocol Buffers (ProtoBuf) focus on describing the \emph{syntactical} structure of a document while others like the \gls{owl} or the \gls{rdf} intend to also define the \emph{semantic} structure of documents as well \parencites{xmlschema}{jsonschema}{avro}{protobuf}{owl}{rdf}.
Furthermore, some technologies like Avro or ProtoBuf do offer other capabilities beyond describing the structure of documents, like describing the structure of \gls{rpc}-based \glspl{api} \parencites{avro}{protobuf}.

% \subsection{Apache Avro}

% Apache Avro is a schema technology for describing the syntactical structure of documents and \gls{rpc}-based \glspl{api}.
Beyond the schema definitions, Avro also provides a compact binary serialization format and support for schema evolution.
Listing \ref{lst:avro-schema-person} shows an example for an Avro schema describing a customer entity.
\parencite{avro}

\begin{listing}[H]
  \inputminted{json}{assets/src/Customer.avsc}
  \caption{Simplified Avro Schema of a Customer Entity}\label{lst:avro-schema-person}
\end{listing}

When a client uses Avro's tooling to serialize a document to binary, Avro expects the document's schema as an input beside the actual data.
The schema is used to write the serialized data sequentially without any type information. 
This makes the resulting byte stream compact but also hard to de-serialize without the document's schema.
The schema with which a document was serialized is called the \emph{writer} schema.
\parencite{avro}

A client that uses Avro to de-serialize a byte stream into a document might expect a certain structure for the document.
That is called the \emph{reader} schema.
Avro expects both the writer and the reader schema as inputs besides the serialized byte stream to de-serialize a document.
\parencite{avro}

In many cases, the writer and reader schema may be equal, yet in some, they might be different.
For example, if an application uses Avro to serialize data to the hard drive and changes its data model and schemas in the meantime.
When the application later reads the data and attempts to de-serialize it, it can use the changed schema as the reader schema but must use the old schema as the writer schema.
Only in this way is Avro able to properly read the serialized data.
If the differences between the schemas are \emph{compatible} Avro can map the data from the reader to the writer schema and successfully de-serialize the data.
If not, then the de-serialization fails.
\parencite{avro}

In this way, Avro supports the compatible evolution of schemas.
Compatible changes are, for example, the adding of a new field with a default value or the removal of a field with a default value.
With Avro's support for field name aliases, fields can even be renamed in a compatible manner.
The Avro project also includes tooling to check the compatibility of two schemas.
\parencite{avro}

% \citeauthor{kreps_kafka_2011} chose Apache Avro as the serialization protocol for their message payloads due to its efficiency and schema evolution capabilities \parencite{kreps_kafka_2011}. 

\section{Schemas in Reactive \acrlongpl{is}}

To examine how schemas can fit into a reactive system, consider the following example:
A web-shop application where users can view the current catalog of products and place orders.
The example omits other aspects of a web-shop for simplicity.
The web-shop's architecture follows the microservice architecture and uses Kafka as a \gls{mom} to be reactive.

\subsection{A Reactive Web-Shop Application}

Figure \ref{fig:web-shop} presents a simple overview of the shop's components and their communication.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\linewidth]{webshop.drawio.png}
  \caption{Simplified Web-Shop Design Using Apache Kafka}\label{fig:web-shop}
\end{figure}

At the top, the \gls{ui} is comprised of two fragments: the catalog view and the checkout.
The catalog view displays the shop's current inventory to the user, fetching it via a synchronous HTTP request from the catalog service.
At the checkout, the user can order items from the catalog, for which the view sends an HTTP request to the order service.

In the backend, the order service processes users' orders (payments, shipping calculation, \ldots) and publishes \texttt{OrderCreated} events to the \texttt{Orders} topic in Kafka.
% Listing \ref{lst:order-created} shows what the Avro schema for such an event might look like.
Please note, \enquote{event} is a term from \gls{ddd} and describes the record of an occurrence like \enquote{The user created an order.}.
In this example, the microservices communicate by recording and sharing the events occurring in their respective business domain by publishing them as messages to Kafka topics.
Therefore, an event is always a message, but a message is not always an event.

The inventory service consumes the events from the \texttt{Orders} topic and updates the shop's stock accordingly by publishing \texttt{StockUpdated} events to the \texttt{Stock} topic.
These events are read by the catalog service, which also reads from the \texttt{Products} topic.
The service joins these two event streams to materialize a view of the current stock combined with product details, which it presents to the catalog view upon request.

\subsection{Benefits of Schemas for Reactive \acrlongpl{is}}\label{sec:schema-benefits}

Introducing Avro schemas to the system can make the web-shop more reactive in several ways.
First, if the microservices serialize the contents of their messages using Avro's compact binary format instead of, for example, UTF8-encoded JSON objects, it would reduce the overall size of the messages.
That would increase the system's throughput of messages, potentially making it more responsive.
It also would make more efficient use of the Kafka broker's disk storage.

Furthermore, the introduction of schemas would formalize the shared understandings about data between microservices.
Instead of informal communications or semi-formal specifications that development teams exchange, schemas represent formal documents that can be checked for soundness, compared, and managed via version control systems.
This reduces the risk of failures occurring due to \enquote{misunderstandings} and makes the system more resilient.
Additionally, when consumers attempt to de-serialize messages which do not conform to their expected schema, the de-serialization fails.
This fail-early mechanism can prevent more critical failures down the line, thereby contributing to the system's resilience.

Lastly, schema evolution capabilities like Avro's can reduce the coupling between microservices introduced by the need to form a shared understanding about the data they exchange.
Consider this example: The order service makes changes to its data model that also affect the \texttt{OrderCreated} events.
If the order service team deploys the service immediately and it starts publishing messages with the new schema, it would break the communication with the inventory service.
Therefore, both services must start to use the new schema simultaneously.
Without a mechanism like feature toggling, the teams must deploy them simultaneously, which negates one of the main advantages of microservices: That they are independently deployable.
Schema evolution removes the coordination effort for making one-sided, compatible data model changes.
That makes microservices more independent, their development teams more autonomous, and the system more flexible.

\subsection{Introducing Schemas to Reactive \acrlongpl{is}}\label{sec:introducing-schemas}

In order to adopt Avro, the development teams have to define schemas for all the messages their services produce or consume.
Listing \ref{lst:order-created} shows what a schema for the \texttt{OrderCreated} event might look like.
Furthermore, each microservice has to use the Avro tooling to (de-)serialize the messages they produce or consume.
Avro offers its functionality as libraries for a variety of programming platforms via the platforms' dependency management tools (Maven, NPM, \ldots) \parencite{avro}.
Services can declare a dependency toward the Avro library for their platform to work with it.

Beyond the functionality of Avro, the microservices also require the actual schemas for the messages they produce or consume so they can implement against the concrete data structures.
Since multiple services work with the same event type, like the order and inventory services, the schemas need to be available in multiple places.
Rather than simply duplicating the schemas in each service's codebase in violation of the \enquote{Don't Repeat Yourself} principle, the schemas should be kept in a single version-controlled repository.
A \gls{cicd} setup can package the schemas into an artifact and publish it to the artifact registry of the appropriate programming platform.
Service can then declare a dependency toward this artifact to consume the schemas.

The described setup can be considered the minimum for integrating Avro into a microservice architecture.
By using Avro to (de-)serialize message payloads, this setup already provides two of the benefits described in \ref{sec:schema-benefits}: reduced message size and formalized communication contracts.
However, this setup does not facilitate schema evolution since every service only has access to the schemas it was compiled with.
That means services have to always use the compile-time schema as both the writer and the reader schema when de-serializing messages.
Schema changes have to be adopted by all services that work with the particular schema simultaneously.
Additionally, services can not de-serialize older messages in the log that were serialized with an older schema version.

Messages need to carry information about their writer schema to facilitate schema evolution.
A simple way of implementing this is to include the writer schema in each message, for example, as a message header.
While being simple, this solution severely restricts the benefit gained from Avro's compact binary format by bloating the message.
Instead, messages could include a reference to the writer schema instead of the whole thing to optimize the approach.
However, where would consumers look up the writer schema based on the reference?
At this point, schema management and schema management solutions come into play.

\section{Schema Management}

Consumers require a way to look up schemas at run-time based on references included in the messages that they consume.
This is the core functional requirement of schema management.
\cite{kreps_kafka_2011}\todo{add other potential citations} address it by introducing a dedicated software component to their system that stores schemas and makes them available.
They refer to this component as a \emph{schema registry}.

A schema registry offers an \gls{api} that allows clients to upload schemas and query for them based on an identifier.
Producers use this \gls{api} to upload the schema of a message to the registry before they publish it.
If the schema does not exist, the registry stores it and generates a unique identifier that it returns to the producer.
Otherwise, if the schema exists, the registry returns the existing identifier.
The producer then includes this identifier in the message's headers.

Consumers read the schema reference from the message before de-serializing its payload.
They use this reference to request the message's writer schema from the registry.
When de-serializing the message, consumers use the fetched writer schema and their compile-time reader schema to utilize Avro's schema evolution capabilities.
Producers and consumers can cache the reference-to-schema mapping locally to reduce the number of requests to the registry.

Beyond the run-time distribution of schemas, schema registries can provide additional functionality that improves the system's resilience.
As mentioned before, Avro provides tooling to check the compatibility of two schemas.
A schema registry that integrates this tooling can provide compatibility checks for new schemas.

An integration of compatibility checks into the schema management setup of the web-shop could look something like the following.
The order service is deployed with incompatible changes to the \texttt{OrderCreated} schema.
It receives a user request to submit an order and, after processing it, attempts to publish an \texttt{OrderCreated} event with the new schema.
Since the service does not have an identifier for the new schema in its cache, it uploads it to the registry.
The registry performs a compatibility check on the new schema using the most recent schema with the same name as the counterpart.
The integrated Avro tooling recognizes that the schemas are incompatible, causing the registry to return an error to the order service's request.
The service logs the error and does not publish the message.

The setup in this example moves the compatibility check from the consumer (as described in \ref{sec:schema-benefits}) to the producer.
It prevents producers from writing incompatible messages to Kafka, keeping the log clean.
The example improves the system's resilience by containing failures due to incompatible changes in the producer instead of spreading them to the consumers.

The web-shop's development setup could also incorporate the schema registry to check schema compatibility even earlier in the development cycle.
The single repository for all schemas described in \ref{sec:introducing-schemas} can be partially replaced by a schema registry.
Schemas can be kept in the repositories of their associated microservices, for example, the \texttt{OrderCreated} schema would be part of the order service's codebase.

A new step for uploading schemas is added to the \gls{cicd} process of all services.
This step may also include compatibility checks, causing the pipeline to fail if incompatible changes were made to the schemas.
Such a setup would prevent incompatible schema changes from even getting deployed.
Services that depend on the schemas of other services, like the inventory service does, can add a step to their \gls{cicd} process to download the schemas from the registry.

\section{Overview of Schema Management Solutions}
